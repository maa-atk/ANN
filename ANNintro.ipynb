{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANNintro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNhSzgbg40Jl5U2BztdBbG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maa-atk/ANN/blob/master/ANNintro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmU9T15n4tnI",
        "colab_type": "text"
      },
      "source": [
        "### **ANN**\n",
        "# **Basic Artificial Neural Network**\n",
        "\n",
        "# Perceptron model \n",
        "\n",
        "\n",
        "Simple functionality then two inputs ,(dentrites) then a function F(x) done on it to get  the output on the (axon).\n",
        "\n",
        "adjust weight ie (multiply the input weights to get it learning) \n",
        "\n",
        "Bias term filter such that the adjusted input must overcome the bias to change the output.\n",
        "\n",
        "Biological neuron to Simple perceptron\n",
        "\n",
        "To get advanced model use multiple layers \n",
        "\n",
        "=>Feed the output of the previous layer to the input of the next layer.\n",
        "\n",
        "=>First layer input layer directy recieves the data input.\n",
        "\n",
        "=>Two or more hidden layers then deep neural network\n",
        "\n",
        "=>Can approximate almost all continuous functions \n",
        "\n",
        "=>Universal Approximation Theorem:= In detail \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDZfbM2TAmef",
        "colab_type": "text"
      },
      "source": [
        "# **Activation Functions:**\n",
        "## (binary classifications)\n",
        "\n",
        " **Note all below activation functions are for binary outputs\n",
        "\n",
        "x*w+b\n",
        "\n",
        "x= input w=weight(how important is the input)\n",
        "\n",
        "b=offset value /threshold (the term x*w must overcome the value of b to actually make a difference on the output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Sigmoid Function\n",
        "=Moderate cutoff smooth \n",
        "=classification binary \n",
        "=probability can be read \n",
        "=sureity can be found \n",
        "=smooth functions\n",
        "=Output lies between 0 and 1\n",
        "f(z)=1/(1+e^(-z))\n",
        "\n",
        "\n",
        "## Hyperbolic function Tangent\n",
        "\n",
        "=Output between -1 to 1\n",
        "\n",
        "\n",
        "\n",
        "## Rectified Linear Unit(ReLU)\n",
        "=if output of value is less than zero then it is zero greater than zero then z\n",
        "max(0,z)\n",
        "\n",
        "=Ideal for vanishing gradient "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PdbGZMYAyvQ",
        "colab_type": "text"
      },
      "source": [
        "# *Activation for a multiclass situation*\n",
        "\n",
        "\n",
        "### **Non exclusive classes :=*\n",
        "One data type can belong to multiple data types \n",
        "\n",
        "0ne hotencoding \n",
        "value of one for all the classes it belongs to and zero for all the classes it does not belong to \n",
        "\n",
        "\n",
        "Multiple categories assigned\n",
        "\n",
        "Can use sigmoid function \n",
        "\n",
        "Probability of the data point         belonging to each category found\n",
        "And later assigned to each category it belongs \n",
        "\n",
        "Independent assignment\n",
        "\n",
        "input =>hidden=> neuron for class1 \n",
        "              => neuron for class2\n",
        "              => neuron for class3\n",
        "\n",
        "\n",
        "\n",
        "### *Mutually exclusive:=\n",
        "Can belong to only one data type \n",
        "\n",
        "one hotencoding \n",
        "value of one for one class zero for all others\n",
        "\n",
        "Single output node for each class \n",
        "\n",
        "\n",
        "input =>hidden=> neuron for class1 \n",
        "              => neuron for class2\n",
        "              => neuron for class3\n",
        "\n",
        "Softmax activation Functions:\n",
        " \n",
        "Sum of all probability is one for all the different neurons.\n",
        "\n",
        "Among the output recieved the highest probability is taken as the output.\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCvfk5gxGJrR",
        "colab_type": "text"
      },
      "source": [
        "## Cost Functions \n",
        "\n",
        "Evaluation of the output:\n",
        "\n",
        "estimated values of the output and comparing with the original data\n",
        "\n",
        "Cost function:\n",
        "\n",
        "measures how off is the output\n",
        "must be an average to compare a single value\n",
        "\n",
        "w*x+b=z\n",
        "\n",
        "pass z to the activation function sigma(z)=a (a is the output holds all info about the weights bias and activation functions )\n",
        "\n",
        "## **Quadratic Cost function**\n",
        "\n",
        "Similar to rmse\n",
        "\n",
        "punishes large errors\n",
        "\n",
        "Depends on:-\n",
        "1.   W=> neural network weights\n",
        "2.   B=>Bias\n",
        "3.   Sr=>input\n",
        "4.   Er=>expected output\n",
        "\n",
        "minimium weight is the min cost function \n",
        "This is gradient descent\n",
        "\n",
        "**Adaptive Gradient Descent:**\n",
        "\n",
        "Learning rate more initially then smaller and smaller till reaching the optimum value.\n",
        "\n",
        "n dim vector => Tensors\n",
        "vector : derivative\n",
        "Tensor : gradient\n",
        "\n",
        "Adam optimizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Cross entrophy Classification:**\n",
        "\n",
        "For classification problems\n",
        "we use this\n",
        "Model predicts a probability distribution of each class\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBbZDn3I2ppE",
        "colab_type": "text"
      },
      "source": [
        "# **BackPropagation**\n",
        "\n",
        "How the results of the cost functions change the layers.\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    }
  ]
}